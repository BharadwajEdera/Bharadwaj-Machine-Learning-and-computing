{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import imageio as im\n",
    "from PIL import Image\n",
    "import os\n",
    "import dlib\n",
    "from imutils.face_utils.helpers import rect_to_bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Bharadwaj\\\\IIST\\\\COMPUTER VISION\\\\Project'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M:\\IIST\\SEM 2\\COMPUTER VISION\\Project\\FaceAttributeRecognition\\glasses_dataset\n"
     ]
    }
   ],
   "source": [
    "cd M:\\IIST\\SEM 2\\COMPUTER VISION\\Project\\FaceAttributeRecognition\\glasses_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive M is MACHINE LEARNING\n",
      " Volume Serial Number is 4E23-4B98\n",
      "\n",
      " Directory of M:\\IIST\\SEM 2\\COMPUTER VISION\\Project\\FaceAttributeRecognition\\glasses_dataset\n",
      "\n",
      "13-02-2021  11:38    <DIR>          .\n",
      "13-02-2021  11:38    <DIR>          ..\n",
      "13-02-2021  11:38    <DIR>          glass\n",
      "13-02-2021  11:38    <DIR>          w_glass\n",
      "               0 File(s)              0 bytes\n",
      "               4 Dir(s)  259,269,644,288 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cropping the faces from images\n",
    "\n",
    "# detector = dlib.get_frontal_face_detector()\n",
    "# dest = 'glass/'\n",
    "# img_dir = ['glasses/']\n",
    "# for im_dir in img_dir:\n",
    "#     images = os.listdir(im_dir)\n",
    "#     for i,image in enumerate(images):\n",
    "#         img = cv2.imread(im_dir + image)\n",
    "#         gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#         rects = detector(gray, 1) \n",
    "#         for j,rect in enumerate(rects):\n",
    "#             (x, y, w, h) = rect_to_bb(rect)\n",
    "# #             cv2.rectangle(img,(x,y),(x+w,y+h),(255,255,255),1)\n",
    "#             sub_face = gray[y:y+h, x:x+w]\n",
    "#             cv2.imwrite(dest+str(i)+'_'+str(j)+'.jpg',sub_face)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 : Reading the Dataset and Resizing each image to 36 * 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 12.21it/s]\n"
     ]
    }
   ],
   "source": [
    "all_images = []\n",
    "labels = []\n",
    "img_dir = ['glass/','w_glass/']\n",
    "for im_dir in tqdm(img_dir):\n",
    "    images = os.listdir(im_dir)\n",
    "    for image in images:\n",
    "        img = cv2.imread(im_dir + image)\n",
    "        img = cv2.resize(img, (36, 36)) # need to resize the image into common size\n",
    "        all_images.append(img)\n",
    "        labels.append(img_dir.index(im_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 75, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread('glass/101_0.jpg')\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 36, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "592"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array(all_images)\n",
    "data = data.astype('float32')\n",
    "data /= 255.0\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(open('glass.npy', 'wb'), data)\n",
    "# np.save(open('labels.npy', 'wb'), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(592, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "labels = np.asarray(labels)\n",
    "labels = labels.reshape(-1, 1)\n",
    "ohe_y=OneHotEncoder()\n",
    "Y=ohe_y.fit_transform(labels)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "x,y = shuffle(data,Y, random_state=42)\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Model for an input image Size of 36 * 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Bharadwaj\\.conda\\envs\\gputest\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(36, 36, 3...)`\n",
      "  \n",
      "C:\\Users\\Bharadwaj\\.conda\\envs\\gputest\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Bharadwaj\\.conda\\envs\\gputest\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\Bharadwaj\\.conda\\envs\\gputest\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Input, Dropout, Convolution2D, MaxPooling2D, Flatten\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "classifier=Sequential()\n",
    "\n",
    "classifier.add(Convolution2D(32,3,3,activation='relu',input_shape=(36, 36, 3)))\n",
    "\n",
    "classifier.add(Convolution2D(64,3,3,activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "classifier.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "classifier.add(Convolution2D(128,3,3,activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "classifier.add(Convolution2D(256,3,3,activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "classifier.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "classifier.add(Flatten())\n",
    "\n",
    "classifier.add(Dense(256,activation='relu'))\n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(Dense(2,activation='softmax'))\n",
    "\n",
    "classifier.compile(Adam(lr=0.0001, decay=1e-6),loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 473 samples, validate on 119 samples\n",
      "Epoch 1/25\n",
      "473/473 [==============================] - 6s 12ms/step - loss: 0.7000 - accuracy: 0.5074 - val_loss: 0.6867 - val_accuracy: 0.4874\n",
      "Epoch 2/25\n",
      "473/473 [==============================] - 5s 11ms/step - loss: 0.5933 - accuracy: 0.6934 - val_loss: 0.4962 - val_accuracy: 0.7815\n",
      "Epoch 3/25\n",
      "473/473 [==============================] - 5s 11ms/step - loss: 0.4275 - accuracy: 0.8288 - val_loss: 0.3590 - val_accuracy: 0.8571\n",
      "Epoch 4/25\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 0.3396 - accuracy: 0.8478 - val_loss: 0.3531 - val_accuracy: 0.8403\n",
      "Epoch 5/25\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 0.3219 - accuracy: 0.8584 - val_loss: 0.3168 - val_accuracy: 0.8655\n",
      "Epoch 6/25\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 0.2560 - accuracy: 0.8985 - val_loss: 0.2808 - val_accuracy: 0.8824\n",
      "Epoch 7/25\n",
      "473/473 [==============================] - 5s 11ms/step - loss: 0.2174 - accuracy: 0.9175 - val_loss: 0.3486 - val_accuracy: 0.8487\n",
      "Epoch 8/25\n",
      "473/473 [==============================] - 5s 11ms/step - loss: 0.2069 - accuracy: 0.9197 - val_loss: 0.2510 - val_accuracy: 0.8992\n",
      "Epoch 9/25\n",
      "473/473 [==============================] - 5s 11ms/step - loss: 0.2029 - accuracy: 0.9112 - val_loss: 0.2485 - val_accuracy: 0.8908\n",
      "Epoch 10/25\n",
      "473/473 [==============================] - 5s 11ms/step - loss: 0.1853 - accuracy: 0.9323 - val_loss: 0.2489 - val_accuracy: 0.8824\n",
      "Epoch 11/25\n",
      "473/473 [==============================] - 5s 11ms/step - loss: 0.1544 - accuracy: 0.9493 - val_loss: 0.2152 - val_accuracy: 0.8992\n",
      "Epoch 12/25\n",
      "473/473 [==============================] - 5s 11ms/step - loss: 0.1506 - accuracy: 0.9408 - val_loss: 0.2481 - val_accuracy: 0.8992\n",
      "Epoch 13/25\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 0.1242 - accuracy: 0.9535 - val_loss: 0.2526 - val_accuracy: 0.8908\n",
      "Epoch 14/25\n",
      "473/473 [==============================] - 5s 11ms/step - loss: 0.1153 - accuracy: 0.9598 - val_loss: 0.2159 - val_accuracy: 0.8908\n",
      "Epoch 15/25\n",
      "473/473 [==============================] - 5s 11ms/step - loss: 0.1143 - accuracy: 0.9598 - val_loss: 0.2285 - val_accuracy: 0.8992\n",
      "Epoch 16/25\n",
      "473/473 [==============================] - 6s 12ms/step - loss: 0.0910 - accuracy: 0.9704 - val_loss: 0.2648 - val_accuracy: 0.8824\n",
      "Epoch 17/25\n",
      "473/473 [==============================] - 5s 11ms/step - loss: 0.0921 - accuracy: 0.9704 - val_loss: 0.2298 - val_accuracy: 0.9076\n",
      "Epoch 18/25\n",
      "473/473 [==============================] - 6s 12ms/step - loss: 0.0889 - accuracy: 0.9683 - val_loss: 0.2518 - val_accuracy: 0.9076\n",
      "Epoch 19/25\n",
      "473/473 [==============================] - 5s 12ms/step - loss: 0.0731 - accuracy: 0.9746 - val_loss: 0.2377 - val_accuracy: 0.8908\n",
      "Epoch 20/25\n",
      "473/473 [==============================] - 6s 13ms/step - loss: 0.0796 - accuracy: 0.9767 - val_loss: 0.4262 - val_accuracy: 0.8655\n",
      "Epoch 21/25\n",
      "473/473 [==============================] - 5s 11ms/step - loss: 0.0544 - accuracy: 0.9767 - val_loss: 0.2465 - val_accuracy: 0.9160\n",
      "Epoch 22/25\n",
      "473/473 [==============================] - 5s 11ms/step - loss: 0.0605 - accuracy: 0.9683 - val_loss: 0.2654 - val_accuracy: 0.8992\n",
      "Epoch 23/25\n",
      "473/473 [==============================] - 5s 11ms/step - loss: 0.0521 - accuracy: 0.9810 - val_loss: 0.2436 - val_accuracy: 0.9076\n",
      "Epoch 24/25\n",
      "473/473 [==============================] - 5s 12ms/step - loss: 0.0460 - accuracy: 0.9852 - val_loss: 0.2729 - val_accuracy: 0.8908\n",
      "Epoch 25/25\n",
      "473/473 [==============================] - 5s 11ms/step - loss: 0.0403 - accuracy: 0.9873 - val_loss: 0.2950 - val_accuracy: 0.8908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x29db1810438>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=25, batch_size=1, shuffle=True, verbose=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 34, 34, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 5, 5, 256)         295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 651,330\n",
      "Trainable params: 651,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Model for an input image Size of 48 * 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = []\n",
    "labels = []\n",
    "img_dir = ['glass/','w_glass/']\n",
    "for im_dir in img_dir:\n",
    "    images = os.listdir(im_dir)\n",
    "    for image in images:\n",
    "        img = cv2.imread(im_dir + image)\n",
    "        img = cv2.resize(img, (48, 48)) # need to resize the image into common size\n",
    "        all_images.append(img)\n",
    "        labels.append(img_dir.index(im_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "592"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array(all_images)\n",
    "data = data.astype('float32')\n",
    "data /= 255.0\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(592, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.save(open('glass.npy', 'wb'), data)\n",
    "# np.save(open('labels.npy', 'wb'), y_train)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "labels = np.asarray(labels)\n",
    "labels = labels.reshape(-1, 1)\n",
    "ohe_y=OneHotEncoder()\n",
    "Y=ohe_y.fit_transform(labels)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bharadwaj\\.conda\\envs\\gputest\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(48, 48, 3...)`\n",
      "  \n",
      "C:\\Users\\Bharadwaj\\.conda\\envs\\gputest\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\Bharadwaj\\.conda\\envs\\gputest\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
      "C:\\Users\\Bharadwaj\\.conda\\envs\\gputest\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 473 samples, validate on 119 samples\n",
      "Epoch 1/25\n",
      "473/473 [==============================] - 12s 25ms/step - loss: 0.6983 - accuracy: 0.5285 - val_loss: 0.6795 - val_accuracy: 0.5126\n",
      "Epoch 2/25\n",
      "473/473 [==============================] - 12s 25ms/step - loss: 0.5451 - accuracy: 0.7188 - val_loss: 0.4108 - val_accuracy: 0.8235\n",
      "Epoch 3/25\n",
      "473/473 [==============================] - 12s 25ms/step - loss: 0.3352 - accuracy: 0.8562 - val_loss: 0.3516 - val_accuracy: 0.8487\n",
      "Epoch 4/25\n",
      "473/473 [==============================] - 12s 26ms/step - loss: 0.2712 - accuracy: 0.8964 - val_loss: 0.2914 - val_accuracy: 0.8824\n",
      "Epoch 5/25\n",
      "473/473 [==============================] - 12s 25ms/step - loss: 0.2364 - accuracy: 0.9091 - val_loss: 0.3240 - val_accuracy: 0.8655\n",
      "Epoch 6/25\n",
      "473/473 [==============================] - 13s 27ms/step - loss: 0.2307 - accuracy: 0.9154 - val_loss: 0.2992 - val_accuracy: 0.8739\n",
      "Epoch 7/25\n",
      "473/473 [==============================] - 12s 26ms/step - loss: 0.1881 - accuracy: 0.9239 - val_loss: 0.2406 - val_accuracy: 0.8908\n",
      "Epoch 8/25\n",
      "473/473 [==============================] - 12s 26ms/step - loss: 0.1802 - accuracy: 0.9281 - val_loss: 0.2519 - val_accuracy: 0.8908\n",
      "Epoch 9/25\n",
      "473/473 [==============================] - 12s 24ms/step - loss: 0.1306 - accuracy: 0.9598 - val_loss: 0.2345 - val_accuracy: 0.8992\n",
      "Epoch 10/25\n",
      "473/473 [==============================] - 12s 25ms/step - loss: 0.1239 - accuracy: 0.9535 - val_loss: 0.2102 - val_accuracy: 0.9160\n",
      "Epoch 11/25\n",
      "473/473 [==============================] - 12s 24ms/step - loss: 0.1052 - accuracy: 0.9641 - val_loss: 0.2583 - val_accuracy: 0.8992\n",
      "Epoch 12/25\n",
      "473/473 [==============================] - 13s 27ms/step - loss: 0.0889 - accuracy: 0.9683 - val_loss: 0.2132 - val_accuracy: 0.9244\n",
      "Epoch 13/25\n",
      "473/473 [==============================] - 12s 25ms/step - loss: 0.0861 - accuracy: 0.9704 - val_loss: 0.2451 - val_accuracy: 0.9076\n",
      "Epoch 14/25\n",
      "473/473 [==============================] - 11s 24ms/step - loss: 0.0994 - accuracy: 0.9641 - val_loss: 0.1991 - val_accuracy: 0.9160\n",
      "Epoch 15/25\n",
      "473/473 [==============================] - 12s 24ms/step - loss: 0.0606 - accuracy: 0.9831 - val_loss: 0.2598 - val_accuracy: 0.8908\n",
      "Epoch 16/25\n",
      "473/473 [==============================] - 12s 25ms/step - loss: 0.0724 - accuracy: 0.9810 - val_loss: 0.1910 - val_accuracy: 0.9244\n",
      "Epoch 17/25\n",
      "473/473 [==============================] - 12s 26ms/step - loss: 0.0502 - accuracy: 0.9810 - val_loss: 0.2188 - val_accuracy: 0.9160\n",
      "Epoch 18/25\n",
      "473/473 [==============================] - 12s 25ms/step - loss: 0.0418 - accuracy: 0.9852 - val_loss: 0.2478 - val_accuracy: 0.9160\n",
      "Epoch 19/25\n",
      "473/473 [==============================] - 12s 25ms/step - loss: 0.0412 - accuracy: 0.9831 - val_loss: 0.3222 - val_accuracy: 0.9076\n",
      "Epoch 20/25\n",
      "473/473 [==============================] - 12s 24ms/step - loss: 0.0431 - accuracy: 0.9873 - val_loss: 0.2544 - val_accuracy: 0.9160\n",
      "Epoch 21/25\n",
      "473/473 [==============================] - 11s 24ms/step - loss: 0.0286 - accuracy: 0.9894 - val_loss: 0.2639 - val_accuracy: 0.9244\n",
      "Epoch 22/25\n",
      "473/473 [==============================] - 12s 25ms/step - loss: 0.0353 - accuracy: 0.9873 - val_loss: 0.3189 - val_accuracy: 0.9076\n",
      "Epoch 23/25\n",
      "473/473 [==============================] - 12s 26ms/step - loss: 0.0198 - accuracy: 0.9915 - val_loss: 0.2576 - val_accuracy: 0.9244\n",
      "Epoch 24/25\n",
      "473/473 [==============================] - 11s 24ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.4809 - val_accuracy: 0.8992\n",
      "Epoch 25/25\n",
      "473/473 [==============================] - 11s 24ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.5540 - val_accuracy: 0.8908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x29daabc1f98>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "x,y = shuffle(data,Y, random_state=42)\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Input, Dropout, Convolution2D, MaxPooling2D, Flatten\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "classifier=Sequential()\n",
    "\n",
    "classifier.add(Convolution2D(32,3,3,activation='relu',input_shape=(48, 48, 3)))\n",
    "\n",
    "classifier.add(Convolution2D(64,3,3,activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "classifier.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "classifier.add(Convolution2D(128,3,3,activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "classifier.add(Convolution2D(256,3,3,activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "classifier.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "classifier.add(Flatten())\n",
    "\n",
    "classifier.add(Dense(256,activation='relu'))\n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(Dense(2,activation='softmax'))\n",
    "\n",
    "classifier.compile(Adam(lr=0.0001, decay=1e-6),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "classifier.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=25, batch_size=1, shuffle=True, verbose=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('glass_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "# target_size = 224\n",
    "# from keras.applications.mobilenetv2 import MobileNetV2\n",
    "# from keras.layers import Dense, Input, Dropout\n",
    "# from keras.models import Model\n",
    "\n",
    "# def build_model( ):\n",
    "#     input_tensor = Input(shape=(target_size, target_size, 3))\n",
    "#     base_model = MobileNetV2(include_top=False, weights='imagenet', input_tensor=input_tensor,\n",
    "#         input_shape=(target_size, target_size, 3), pooling='avg')\n",
    "\n",
    "#     for layer in base_model.layers:\n",
    "#         layer.trainable = True  # trainable has to be false in order to freeze the layers\n",
    "#     op = Dense(1, activation = 'softmax')(base_model.output)\n",
    "\n",
    "#     output_tensor = Dense(1, activation='softmax')(op)\n",
    "\n",
    "#     model = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "#     return model\n",
    "\n",
    "# from keras.optimizers import Adam\n",
    "# model = build_model()\n",
    "# model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model.fit(X_train, y_train, epochs = 5, verbose = 1,batch_size=1 ,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# renaming the images in to the new directory\n",
    "\n",
    "source ='images/'          \n",
    "dest = 'men/'\n",
    "images = sorted(os.listdir(source))\n",
    "k=603\n",
    "for image in images:\n",
    "    print(image)\n",
    "    img  = cv2.imread(os.path.join(source,image))\n",
    "    cv2.imshow('image',img)\n",
    "    cv2.waitKey(1000)\n",
    "    img = cv2.resize(img, (224, 224)) # need to resize the image into common size\n",
    "    extension = image.split('.')[-1]\n",
    "    k+=1\n",
    "    cv2.imwrite(dest+str(k)+'.'+extension,img)\n",
    "\n",
    "len(images)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-gputest] *",
   "language": "python",
   "name": "conda-env-.conda-gputest-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
